{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_shakespeare = tf.keras.utils.get_file('shakespeare.txt', 'https://www.gutenberg.org/files/100/100-0.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_shakespeare, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740054"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of chars\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_shakespeare(text):\n",
    "    # get contents\n",
    "    contents = text[974:2893].replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    contents = re.sub(\"\\s{2,}\", \",,\", contents)\n",
    "    contents = contents.split(\",,\")\n",
    "    \n",
    "    for idx,book in enumerate(contents):\n",
    "        if book == \"THE LIFE OF KING HENRY THE FIFTH\":\n",
    "            contents[idx] = \"THE LIFE OF KING HENRY V\"\n",
    "        elif book == \"THE TRAGEDY OF MACBETH\":\n",
    "            contents[idx] = \"MACBETH\"\n",
    "        elif book == \"THE TRAGEDY OF OTHELLO, MOOR OF VENICE\":\n",
    "            contents[idx] = \"OTHELLO, THE MOOR OF VENICE\"\n",
    "        elif book == \"TWELFTH NIGHT; OR, WHAT YOU WILL\":\n",
    "            contents[idx] = \"TWELFTH NIGHT: OR, WHAT YOU WILL\"\n",
    "    \n",
    "    # remove the project gutenberg info\n",
    "    text = text[2893:-21529]\n",
    "    \n",
    "    shakespeare_dict = {}\n",
    "    \n",
    "    for idx,book in enumerate(contents):\n",
    "        strpos = text.find(book) + len(book)\n",
    "        \n",
    "        if idx + 1 != len(contents):\n",
    "            next_book = contents[idx + 1]\n",
    "            endpos = text.find(next_book, strpos)\n",
    "            shakespeare_dict[book] = text[strpos:endpos]\n",
    "        else:\n",
    "            shakespeare_dict[book] = text[strpos:]\n",
    "    \n",
    "    return shakespeare_dict, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "books, text = clean_shakespeare(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTHE SONNETS\\r\\n\\r\\n                    1\\r\\n\\r\\nFrom fairest creatures we desire increase,\\r\\nTh'\n",
      "Target data: '\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTHE SONNETS\\r\\n\\r\\n                    1\\r\\n\\r\\nFrom fairest creatures we desire increase,\\r\\nTha'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 2 ('\\r')\n",
      "  expected output: 1 ('\\n')\n",
      "Step    1\n",
      "  input: 1 ('\\n')\n",
      "  expected output: 2 ('\\r')\n",
      "Step    2\n",
      "  input: 2 ('\\r')\n",
      "  expected output: 1 ('\\n')\n",
      "Step    3\n",
      "  input: 1 ('\\n')\n",
      "  expected output: 2 ('\\r')\n",
      "Step    4\n",
      "  input: 2 ('\\r')\n",
      "  expected output: 1 ('\\n')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 884 steps\n",
      "Epoch 1/30\n",
      "884/884 [==============================] - 2641s 3s/step - loss: 1.8918\n",
      "Epoch 2/30\n",
      "884/884 [==============================] - 2704s 3s/step - loss: 1.3783\n",
      "Epoch 3/30\n",
      "884/884 [==============================] - 2444s 3s/step - loss: 1.2907\n",
      "Epoch 4/30\n",
      "884/884 [==============================] - 2609s 3s/step - loss: 1.2474\n",
      "Epoch 5/30\n",
      "884/884 [==============================] - 3644s 4s/step - loss: 1.2181\n",
      "Epoch 6/30\n",
      "884/884 [==============================] - 2430s 3s/step - loss: 1.1959\n",
      "Epoch 7/30\n",
      "884/884 [==============================] - 2188s 2s/step - loss: 1.1784\n",
      "Epoch 8/30\n",
      "884/884 [==============================] - 2490s 3s/step - loss: 1.1636\n",
      "Epoch 9/30\n",
      "884/884 [==============================] - 2773s 3s/step - loss: 1.1514\n",
      "Epoch 10/30\n",
      "884/884 [==============================] - 2655s 3s/step - loss: 1.1412\n",
      "Epoch 11/30\n",
      "884/884 [==============================] - 2614s 3s/step - loss: 1.1327\n",
      "Epoch 12/30\n",
      "884/884 [==============================] - 3736s 4s/step - loss: 1.1260\n",
      "Epoch 13/30\n",
      "884/884 [==============================] - 2560s 3s/step - loss: 1.1203\n",
      "Epoch 14/30\n",
      "884/884 [==============================] - 2029s 2s/step - loss: 1.1165\n",
      "Epoch 15/30\n",
      "884/884 [==============================] - 2031s 2s/step - loss: 1.1140\n",
      "Epoch 16/30\n",
      "884/884 [==============================] - 2036s 2s/step - loss: 1.1119\n",
      "Epoch 17/30\n",
      "884/884 [==============================] - 2037s 2s/step - loss: 1.1101\n",
      "Epoch 18/30\n",
      "884/884 [==============================] - 2040s 2s/step - loss: 1.1098\n",
      "Epoch 19/30\n",
      "884/884 [==============================] - 2039s 2s/step - loss: 1.1111\n",
      "Epoch 20/30\n",
      "884/884 [==============================] - 2035s 2s/step - loss: 1.1122\n",
      "Epoch 21/30\n",
      "884/884 [==============================] - 2028s 2s/step - loss: 1.1145\n",
      "Epoch 22/30\n",
      "884/884 [==============================] - 2031s 2s/step - loss: 1.1197\n",
      "Epoch 23/30\n",
      "884/884 [==============================] - 2031s 2s/step - loss: 1.1214\n",
      "Epoch 24/30\n",
      "884/884 [==============================] - 2030s 2s/step - loss: 1.1258\n",
      "Epoch 25/30\n",
      "884/884 [==============================] - 2042s 2s/step - loss: 1.1319\n",
      "Epoch 26/30\n",
      "884/884 [==============================] - 2037s 2s/step - loss: 1.1401\n",
      "Epoch 27/30\n",
      "884/884 [==============================] - 2093s 2s/step - loss: 1.1491\n",
      "Epoch 28/30\n",
      "884/884 [==============================] - 2353s 3s/step - loss: 1.1637\n",
      "Epoch 29/30\n",
      "884/884 [==============================] - 2095s 2s/step - loss: 1.1692\n",
      "Epoch 30/30\n",
      "884/884 [==============================] - 2140s 2s/step - loss: 1.1825\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            26112     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 102)            104550    \n",
      "=================================================================\n",
      "Total params: 4,068,966\n",
      "Trainable params: 4,068,966\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rose fire;\n",
      "  But for the hair of mothers, in the burs,\n",
      "    Others to any day 'tis well. Grant things can water,\n",
      "    I desire you attate yourself to gaze,\n",
      "  And yet I had a broad, and pity life,\n",
      "  And to be taken foolery that I met have beat, sirreiver, have I poison’d;\n",
      "Reignith in how sbear thy way; and from—\n",
      "Cold of my service, the youth does of the sea\n",
      "That makes us not set in my best worth:\n",
      "Her age with her by papient Boy, in all my fool,\n",
      "I hie the salder by my death, I naked,\n",
      "Look do’t barnes. Bo true, my lord you must.\n",
      "\n",
      "Possessel out of a demuner rect,\n",
      "Nor couplete as Anne, but we are entereat’st\n",
      "than young in comfort, my lord!\n",
      "\n",
      "VIOLA.\n",
      "I say.\n",
      "\n",
      "RODERIGO.\n",
      "There, lady, since my true-piontine,\n",
      "    As clap away. Let's not ask solicit,\n",
      "And the most unfatificuate Romans of honourablic you,\n",
      "Yet long keeps in a bark up witchcreful) else\n",
      "That I am sure\n",
      "That measur’d strangeness labours should be foulor,\n",
      "  Procostant many Fantastic have founded.\n",
      "My sentence, Catca, and\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"A rose \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-Unit-4-Sprint-3-Deep-Learning",
   "language": "python",
   "name": "ds-unit-4-sprint-3-deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
